{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by date and Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH = '2020-09'\n",
    "INPUT_DIR = f'/data/crawler/trade-{MONTH}'\n",
    "OUTPUT_DIR = f'/data/split/trade-{MONTH}'\n",
    "SORTED_DIR = f'/data/json/trade-{MONTH}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(INPUT_DIR)\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "if os.path.exists(SORTED_DIR):\n",
    "    shutil.rmtree(SORTED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(input_file:str, output_dir:str)->None:\n",
    "    ss = os.path.basename(input_file).split('.')\n",
    "    exchange_global = ss[0]\n",
    "    market_type_global = ss[1]\n",
    "    pair_global = ss[2]\n",
    "\n",
    "    if input_file.endswith('.json.gz'):\n",
    "        f = gzip.open(input_file, 'rt')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    elif input_file.endswith('.zip'):\n",
    "        try:\n",
    "            zf = zipfile.ZipFile(input_file, 'r')\n",
    "        except zipfile.BadZipFile:\n",
    "            # raise ValueError(input_file)\n",
    "            return\n",
    "        assert len(zf.namelist()) == 1\n",
    "        lines = zf.read(zf.namelist()[0]).decode('UTF-8').split('\\n')\n",
    "        zf.close()\n",
    "    elif input_file.endswith('.json') or input_file.endswith('file.log'):\n",
    "        f = open(input_file, 'rt')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else:\n",
    "        raise ValueError('Unknown file suffix ' + input_file)\n",
    "\n",
    "    output_file_pool = {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as ex:\n",
    "                continue  # TODO: why !!!\n",
    "            if obj['exchange'] == 'Bitfinex' and obj['marketType'] == 'Futures':\n",
    "                obj['marketType'] = 'Swap'  # bugfix for Bitfinex\n",
    "                line = json.dumps(obj)\n",
    "            elif obj['exchange'] == 'WhaleEx' and obj['trade_id'] != obj['raw']['tradeId']:\n",
    "                obj['trade_id'] = str(obj['raw']['tradeId'])  # bugfix for WhaleEx\n",
    "                line = json.dumps(obj)\n",
    "\n",
    "            date_str = datetime.datetime.fromtimestamp(obj['timestamp']/1000.0).isoformat()[0:10]\n",
    "            if date_str[:-3] != MONTH:\n",
    "                continue  # discard timeout records\n",
    "\n",
    "            exchange = obj['exchange']\n",
    "            market_type = obj['marketType']\n",
    "            pair = obj['pair']\n",
    "            assert exchange == exchange_global\n",
    "            assert market_type == market_type_global\n",
    "            assert pair == pair_global\n",
    "            rawPair = obj['rawPair']\n",
    "            filename = f'{exchange}.{market_type}.{pair}.{rawPair}' if market_type == 'Futures' else f'{exchange}.{market_type}.{pair}'\n",
    "\n",
    "            output_file = os.path.join(output_dir, f'{filename}.{date_str}.json')\n",
    "            if output_file in output_file_pool:\n",
    "                file_object = output_file_pool[output_file]\n",
    "            else:\n",
    "                file_object = open(output_file, 'at')\n",
    "                output_file_pool[output_file] = file_object\n",
    "            file_object.write(line + '\\n')\n",
    "\n",
    "    for file, file_object in output_file_pool.items():\n",
    "        file_object.close()\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split(f'{INPUT_DIR}/Huobi.Spot.EOS_USDT.2020-06-21.zip', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_one_group(csv_files: List[str], output_dir: str)->None:\n",
    "    '''Work on files within the same exchange, market_type and pair in a single process.'''\n",
    "    for file in csv_files:\n",
    "        split(file, output_dir)\n",
    "\n",
    "def split_multi(input_dir:str, output_dir:str)->None:\n",
    "    json_files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.json\"), recursive=True)]\n",
    "    zip_files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.zip\"), recursive=True)]\n",
    "    gz_files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.json.gz\"), recursive=True)]\n",
    "    log_files = [f for f in glob.glob(os.path.join(input_dir, \"**/file.log\"), recursive=True)]\n",
    "    files = json_files+zip_files+gz_files+log_files\n",
    "\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    # exchange_market_pair -> files\n",
    "    exchange_market_pair = {}\n",
    "    for file in files:\n",
    "        ss = os.path.basename(file).split('.')\n",
    "        exchange = ss[0]\n",
    "        market_type = ss[1]\n",
    "        pair = ss[2]\n",
    "        key = f'{exchange}.{market_type}.{pair}'\n",
    "        if key not in exchange_market_pair:\n",
    "            exchange_market_pair[key] = []\n",
    "        exchange_market_pair[key].append(file)\n",
    "\n",
    "    with ProgressBar():\n",
    "        db.from_sequence(list(exchange_market_pair.keys())).map(lambda key: split_one_group(exchange_market_pair[key], output_dir)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 19min 19.6s\n"
     ]
    }
   ],
   "source": [
    "split_multi(INPUT_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_and_sort(input_file:str, ouput_file:str)->None:\n",
    "    trade_map = {};\n",
    "    f = open(input_file, 'rt')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            obj = json.loads(line)\n",
    "            exchange = obj['exchange']\n",
    "            market_type = obj['marketType']\n",
    "            pair = obj['pair']\n",
    "            raw_pair = obj['rawPair']\n",
    "            trade_id = obj['trade_id']\n",
    "            if not trade_id:  # Fix trade_id for Kraken, MXC\n",
    "                if exchange == 'Kraken' or exchange == 'MXC':\n",
    "                    obj['trade_id'] = str(obj['timestamp'])\n",
    "                    trade_id = obj['trade_id']\n",
    "                    line = json.dumps(obj)\n",
    "            if not trade_id:\n",
    "                f.close()\n",
    "                raise ValueError(line)\n",
    "            key = f'{exchange}-{market_type}-{pair}-{raw_pair}-{trade_id}'\n",
    "            trade_map[key] = {'line': line, 'sort_key': int(trade_id) if exchange!='BitMEX' else obj['timestamp']}\n",
    "    f.close()\n",
    "\n",
    "    trade_array = list(trade_map.values())\n",
    "    del trade_map\n",
    "    trade_array.sort(key=lambda x: x['sort_key'])\n",
    "\n",
    "    f = open(ouput_file, 'wt')\n",
    "    for item in trade_array:\n",
    "        f.write(item['line'] + '\\n')\n",
    "    del trade_array\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_and_sort_wrapper(input_file:str, output_dir:str)->None:\n",
    "    date_str = input_file.split('.')[-2]\n",
    "    date_dir = os.path.join(output_dir, date_str)\n",
    "    os.makedirs(date_dir, exist_ok = True)\n",
    "    dedup_and_sort(input_file, os.path.join(date_dir, os.path.basename(input_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_sort_multi(input_dir:str, output_dir:str)->None:\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "    files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.json\"), recursive=True)]\n",
    "    with ProgressBar():\n",
    "        db.from_sequence(files).map(lambda file: dedup_and_sort_wrapper(file, output_dir)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11min 49.4s\n"
     ]
    }
   ],
   "source": [
    "dedup_sort_multi(OUTPUT_DIR, SORTED_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
